{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from statistics import median\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data as Data\n",
    "torch.manual_seed(1)    # reproducible\n",
    "from model import HierAttnNet \n",
    "from tqdm import trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def load_x():\n",
    "    \n",
    "    doc_list = []\n",
    "    with open('./yct/train_data3.tsv') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            doc_list.append(row[0])\n",
    "    \n",
    "    return doc_list\n",
    "\n",
    "def onehot_y():\n",
    "    \n",
    "    mapping = {}\n",
    "    order = 1\n",
    "    y_cate_list = []\n",
    "    \n",
    "    with open('./yct/train_data3.tsv') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            cate = row[1]\n",
    "            if cate not in mapping:\n",
    "                mapping[cate] = order\n",
    "                y_cate_list.append(order)\n",
    "                order += 1\n",
    "            else:\n",
    "                y_cate_list.append(mapping[cate])\n",
    "    return y_cate_list, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "docs = load_x()\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "max_length = 15\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length)\n",
    "#print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "epochs = 50\n",
    "vali_ratio = 0.01\n",
    "batch_size = 64\n",
    "optimizer_str = 'Adam'\n",
    "default_dropout_rate = 0.2\n",
    "hidden_dim = 70\n",
    "\n",
    "\n",
    "X_seq = np.array(padded_docs)\n",
    "ycate = np.array(onehot_y()[0])\n",
    "\n",
    "# train valid split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_seq, ycate, random_state=127, test_size = vali_ratio)\n",
    "\n",
    "\n",
    "# put data to dataloader (gpu version)\n",
    "# x_train = torch.from_numpy(x_train).to(torch.long).cuda()\n",
    "# y_train = torch.from_numpy(y_train).to(torch.long).cuda()\n",
    "# x_valid = torch.from_numpy(x_valid).to(torch.long).cuda()\n",
    "# y_valid = torch.from_numpy(y_valid).to(torch.long).cuda()\n",
    "\n",
    "# put data to dataloader (cpu version)\n",
    "x_train = torch.from_numpy(x_train).to(torch.long)\n",
    "y_train = torch.from_numpy(y_train).to(torch.long)\n",
    "x_valid = torch.from_numpy(x_valid).to(torch.long)\n",
    "y_valid = torch.from_numpy(y_valid).to(torch.long)\n",
    "\n",
    "\n",
    "# 轉torch 能識別的 Dataset\n",
    "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=batch_size,      # mini batch size\n",
    "        shuffle=True,           \n",
    "        num_workers=8,             # only cpu\n",
    "        pin_memory=True            # only cpu\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = HierAttnNet(\n",
    "        doc_len=x_train.shape[1],  \n",
    "        embedding_dim=150,\n",
    "        hidden_dim=hidden_dim,\n",
    "        vocab_size=vocab_size,\n",
    "        tagset_size=onehot_y()[1],\n",
    "        default_dropout_rate=default_dropout_rate,\n",
    "        embedding_pretrained=None,\n",
    "        embedding_freeze=False)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "if optimizer_str == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "elif optimizer_str == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "else:\n",
    "    raise 'optimizer_str is not set correctly!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 0, Step: 0:\n",
      "Epoch: 0, Step: 100:\n",
      "Epoch: 0, Step: 200:\n",
      "Epoch: 0, Step: 300:\n",
      "Epoch: 0, Step: 400:\n",
      "Epoch: 0, Step: 500:\n",
      "Epoch: 0, Step: 600:\n",
      "Epoch: 1, Step: 0:\n",
      "Epoch: 1, Step: 100:\n",
      "Epoch: 1, Step: 200:\n",
      "Epoch: 1, Step: 300:\n",
      "Epoch: 1, Step: 400:\n",
      "Epoch: 1, Step: 500:\n",
      "Epoch: 1, Step: 600:\n",
      "Epoch: 2, Step: 0:\n",
      "Epoch: 2, Step: 100:\n",
      "Epoch: 2, Step: 200:\n",
      "Epoch: 2, Step: 300:\n",
      "Epoch: 2, Step: 400:\n",
      "Epoch: 2, Step: 500:\n",
      "Epoch: 2, Step: 600:\n",
      "Epoch: 3, Step: 0:\n",
      "Epoch: 3, Step: 100:\n",
      "Epoch: 3, Step: 200:\n",
      "Epoch: 3, Step: 300:\n",
      "Epoch: 3, Step: 400:\n",
      "Epoch: 3, Step: 500:\n",
      "Epoch: 3, Step: 600:\n",
      "Epoch: 4, Step: 0:\n",
      "Epoch: 4, Step: 100:\n",
      "Epoch: 4, Step: 200:\n",
      "Epoch: 4, Step: 300:\n",
      "Epoch: 4, Step: 400:\n",
      "Epoch: 4, Step: 500:\n",
      "Epoch: 4, Step: 600:\n",
      "Epoch: 5, Step: 0:\n",
      "Epoch: 5, Step: 100:\n",
      "Epoch: 5, Step: 200:\n",
      "Epoch: 5, Step: 300:\n",
      "Epoch: 5, Step: 400:\n",
      "Epoch: 5, Step: 500:\n",
      "Epoch: 5, Step: 600:\n",
      "Epoch: 6, Step: 0:\n",
      "Epoch: 6, Step: 100:\n",
      "Epoch: 6, Step: 200:\n",
      "Epoch: 6, Step: 300:\n",
      "Epoch: 6, Step: 400:\n",
      "Epoch: 6, Step: 500:\n",
      "Epoch: 6, Step: 600:\n",
      "Epoch: 7, Step: 0:\n",
      "Epoch: 7, Step: 100:\n",
      "Epoch: 7, Step: 200:\n",
      "Epoch: 7, Step: 300:\n",
      "Epoch: 7, Step: 400:\n",
      "Epoch: 7, Step: 500:\n",
      "Epoch: 7, Step: 600:\n",
      "Epoch: 8, Step: 0:\n",
      "Epoch: 8, Step: 100:\n",
      "Epoch: 8, Step: 200:\n",
      "Epoch: 8, Step: 300:\n",
      "Epoch: 8, Step: 400:\n",
      "Epoch: 8, Step: 500:\n",
      "Epoch: 8, Step: 600:\n",
      "Epoch: 9, Step: 0:\n",
      "Epoch: 9, Step: 100:\n",
      "Epoch: 9, Step: 200:\n",
      "Epoch: 9, Step: 300:\n",
      "Epoch: 9, Step: 400:\n",
      "Epoch: 9, Step: 500:\n",
      "Epoch: 9, Step: 600:\n",
      "Epoch: 10, Step: 0:\n",
      "Epoch: 10, Step: 100:\n",
      "Epoch: 10, Step: 200:\n",
      "Epoch: 10, Step: 300:\n",
      "Epoch: 10, Step: 400:\n",
      "Epoch: 10, Step: 500:\n",
      "Epoch: 10, Step: 600:\n",
      "Epoch: 11, Step: 0:\n",
      "Epoch: 11, Step: 100:\n",
      "Epoch: 11, Step: 200:\n",
      "Epoch: 11, Step: 300:\n",
      "Epoch: 11, Step: 400:\n",
      "Epoch: 11, Step: 500:\n",
      "Epoch: 11, Step: 600:\n",
      "Epoch: 12, Step: 0:\n",
      "Epoch: 12, Step: 100:\n",
      "Epoch: 12, Step: 200:\n",
      "Epoch: 12, Step: 300:\n",
      "Epoch: 12, Step: 400:\n",
      "Epoch: 12, Step: 500:\n",
      "Epoch: 12, Step: 600:\n",
      "Epoch: 13, Step: 0:\n",
      "Epoch: 13, Step: 100:\n",
      "Epoch: 13, Step: 200:\n",
      "Epoch: 13, Step: 300:\n",
      "Epoch: 13, Step: 400:\n",
      "Epoch: 13, Step: 500:\n",
      "Epoch: 13, Step: 600:\n",
      "Epoch: 14, Step: 0:\n",
      "Epoch: 14, Step: 100:\n",
      "Epoch: 14, Step: 200:\n",
      "Epoch: 14, Step: 300:\n",
      "Epoch: 14, Step: 400:\n",
      "Epoch: 14, Step: 500:\n",
      "Epoch: 14, Step: 600:\n",
      "Epoch: 15, Step: 0:\n",
      "Epoch: 15, Step: 100:\n",
      "Epoch: 15, Step: 200:\n",
      "Epoch: 15, Step: 300:\n",
      "Epoch: 15, Step: 400:\n",
      "Epoch: 15, Step: 500:\n",
      "Epoch: 15, Step: 600:\n",
      "Epoch: 16, Step: 0:\n",
      "Epoch: 16, Step: 100:\n",
      "Epoch: 16, Step: 200:\n",
      "Epoch: 16, Step: 300:\n",
      "Epoch: 16, Step: 400:\n",
      "Epoch: 16, Step: 500:\n",
      "Epoch: 16, Step: 600:\n",
      "Epoch: 17, Step: 0:\n",
      "Epoch: 17, Step: 100:\n",
      "Epoch: 17, Step: 200:\n",
      "Epoch: 17, Step: 300:\n",
      "Epoch: 17, Step: 400:\n",
      "Epoch: 17, Step: 500:\n",
      "Epoch: 17, Step: 600:\n",
      "Epoch: 18, Step: 0:\n",
      "Epoch: 18, Step: 100:\n",
      "Epoch: 18, Step: 200:\n",
      "Epoch: 18, Step: 300:\n",
      "Epoch: 18, Step: 400:\n",
      "Epoch: 18, Step: 500:\n",
      "Epoch: 18, Step: 600:\n",
      "Epoch: 19, Step: 0:\n",
      "Epoch: 19, Step: 100:\n",
      "Epoch: 19, Step: 200:\n",
      "Epoch: 19, Step: 300:\n",
      "Epoch: 19, Step: 400:\n",
      "Epoch: 19, Step: 500:\n",
      "Epoch: 19, Step: 600:\n",
      "Epoch: 20, Step: 0:\n",
      "Epoch: 20, Step: 100:\n",
      "Epoch: 20, Step: 200:\n",
      "Epoch: 20, Step: 300:\n",
      "Epoch: 20, Step: 400:\n",
      "Epoch: 20, Step: 500:\n",
      "Epoch: 20, Step: 600:\n",
      "Epoch: 21, Step: 0:\n",
      "Epoch: 21, Step: 100:\n",
      "Epoch: 21, Step: 200:\n",
      "Epoch: 21, Step: 300:\n",
      "Epoch: 21, Step: 400:\n",
      "Epoch: 21, Step: 500:\n",
      "Epoch: 21, Step: 600:\n",
      "Epoch: 22, Step: 0:\n",
      "Epoch: 22, Step: 100:\n",
      "Epoch: 22, Step: 200:\n",
      "Epoch: 22, Step: 300:\n",
      "Epoch: 22, Step: 400:\n",
      "Epoch: 22, Step: 500:\n",
      "Epoch: 22, Step: 600:\n",
      "Epoch: 23, Step: 0:\n",
      "Epoch: 23, Step: 100:\n",
      "Epoch: 23, Step: 200:\n",
      "Epoch: 23, Step: 300:\n",
      "Epoch: 23, Step: 400:\n",
      "Epoch: 23, Step: 500:\n",
      "Epoch: 23, Step: 600:\n",
      "Epoch: 24, Step: 0:\n",
      "Epoch: 24, Step: 100:\n",
      "Epoch: 24, Step: 200:\n",
      "Epoch: 24, Step: 300:\n",
      "Epoch: 24, Step: 400:\n",
      "Epoch: 24, Step: 500:\n",
      "Epoch: 24, Step: 600:\n",
      "Epoch: 25, Step: 0:\n",
      "Epoch: 25, Step: 100:\n",
      "Epoch: 25, Step: 200:\n",
      "Epoch: 25, Step: 300:\n",
      "Epoch: 25, Step: 400:\n",
      "Epoch: 25, Step: 500:\n",
      "Epoch: 25, Step: 600:\n",
      "Epoch: 26, Step: 0:\n",
      "Epoch: 26, Step: 100:\n",
      "Epoch: 26, Step: 200:\n",
      "Epoch: 26, Step: 300:\n",
      "Epoch: 26, Step: 400:\n",
      "Epoch: 26, Step: 500:\n",
      "Epoch: 26, Step: 600:\n",
      "Epoch: 27, Step: 0:\n",
      "Epoch: 27, Step: 100:\n",
      "Epoch: 27, Step: 200:\n",
      "Epoch: 27, Step: 300:\n",
      "Epoch: 27, Step: 400:\n",
      "Epoch: 27, Step: 500:\n",
      "Epoch: 27, Step: 600:\n",
      "Epoch: 28, Step: 0:\n",
      "Epoch: 28, Step: 100:\n",
      "Epoch: 28, Step: 200:\n",
      "Epoch: 28, Step: 300:\n",
      "Epoch: 28, Step: 400:\n",
      "Epoch: 28, Step: 500:\n",
      "Epoch: 28, Step: 600:\n",
      "Epoch: 29, Step: 0:\n",
      "Epoch: 29, Step: 100:\n",
      "Epoch: 29, Step: 200:\n",
      "Epoch: 29, Step: 300:\n",
      "Epoch: 29, Step: 400:\n",
      "Epoch: 29, Step: 500:\n",
      "Epoch: 29, Step: 600:\n",
      "Epoch: 30, Step: 0:\n",
      "Epoch: 30, Step: 100:\n",
      "Epoch: 30, Step: 200:\n",
      "Epoch: 30, Step: 300:\n",
      "Epoch: 30, Step: 400:\n",
      "Epoch: 30, Step: 500:\n",
      "Epoch: 30, Step: 600:\n",
      "Epoch: 31, Step: 0:\n",
      "Epoch: 31, Step: 100:\n",
      "Epoch: 31, Step: 200:\n",
      "Epoch: 31, Step: 300:\n",
      "Epoch: 31, Step: 400:\n",
      "Epoch: 31, Step: 500:\n",
      "Epoch: 31, Step: 600:\n",
      "Epoch: 32, Step: 0:\n",
      "Epoch: 32, Step: 100:\n",
      "Epoch: 32, Step: 200:\n",
      "Epoch: 32, Step: 300:\n",
      "Epoch: 32, Step: 400:\n",
      "Epoch: 32, Step: 500:\n",
      "Epoch: 32, Step: 600:\n",
      "Epoch: 33, Step: 0:\n",
      "Epoch: 33, Step: 100:\n",
      "Epoch: 33, Step: 200:\n",
      "Epoch: 33, Step: 300:\n",
      "Epoch: 33, Step: 400:\n",
      "Epoch: 33, Step: 500:\n",
      "Epoch: 33, Step: 600:\n",
      "Epoch: 34, Step: 0:\n",
      "Epoch: 34, Step: 100:\n",
      "Epoch: 34, Step: 200:\n",
      "Epoch: 34, Step: 300:\n",
      "Epoch: 34, Step: 400:\n",
      "Epoch: 34, Step: 500:\n",
      "Epoch: 34, Step: 600:\n",
      "Epoch: 35, Step: 0:\n",
      "Epoch: 35, Step: 100:\n",
      "Epoch: 35, Step: 200:\n",
      "Epoch: 35, Step: 300:\n",
      "Epoch: 35, Step: 400:\n",
      "Epoch: 35, Step: 500:\n",
      "Epoch: 35, Step: 600:\n",
      "Epoch: 36, Step: 0:\n",
      "Epoch: 36, Step: 100:\n",
      "Epoch: 36, Step: 200:\n",
      "Epoch: 36, Step: 300:\n",
      "Epoch: 36, Step: 400:\n",
      "Epoch: 36, Step: 500:\n",
      "Epoch: 36, Step: 600:\n",
      "Epoch: 37, Step: 0:\n",
      "Epoch: 37, Step: 100:\n",
      "Epoch: 37, Step: 200:\n",
      "Epoch: 37, Step: 300:\n",
      "Epoch: 37, Step: 400:\n",
      "Epoch: 37, Step: 500:\n",
      "Epoch: 37, Step: 600:\n",
      "Epoch: 38, Step: 0:\n",
      "Epoch: 38, Step: 100:\n",
      "Epoch: 38, Step: 200:\n",
      "Epoch: 38, Step: 300:\n",
      "Epoch: 38, Step: 400:\n",
      "Epoch: 38, Step: 500:\n",
      "Epoch: 38, Step: 600:\n",
      "Epoch: 39, Step: 0:\n",
      "Epoch: 39, Step: 100:\n",
      "Epoch: 39, Step: 200:\n",
      "Epoch: 39, Step: 300:\n",
      "Epoch: 39, Step: 400:\n",
      "Epoch: 39, Step: 500:\n",
      "Epoch: 39, Step: 600:\n",
      "Epoch: 40, Step: 0:\n",
      "Epoch: 40, Step: 100:\n",
      "Epoch: 40, Step: 200:\n",
      "Epoch: 40, Step: 300:\n",
      "Epoch: 40, Step: 400:\n",
      "Epoch: 40, Step: 500:\n",
      "Epoch: 40, Step: 600:\n",
      "Epoch: 41, Step: 0:\n",
      "Epoch: 41, Step: 100:\n",
      "Epoch: 41, Step: 200:\n",
      "Epoch: 41, Step: 300:\n",
      "Epoch: 41, Step: 400:\n",
      "Epoch: 41, Step: 500:\n",
      "Epoch: 41, Step: 600:\n",
      "Epoch: 42, Step: 0:\n",
      "Epoch: 42, Step: 100:\n",
      "Epoch: 42, Step: 200:\n",
      "Epoch: 42, Step: 300:\n",
      "Epoch: 42, Step: 400:\n",
      "Epoch: 42, Step: 500:\n",
      "Epoch: 42, Step: 600:\n",
      "Epoch: 43, Step: 0:\n",
      "Epoch: 43, Step: 100:\n",
      "Epoch: 43, Step: 200:\n",
      "Epoch: 43, Step: 300:\n",
      "Epoch: 43, Step: 400:\n",
      "Epoch: 43, Step: 500:\n",
      "Epoch: 43, Step: 600:\n",
      "Epoch: 44, Step: 0:\n",
      "Epoch: 44, Step: 100:\n",
      "Epoch: 44, Step: 200:\n",
      "Epoch: 44, Step: 300:\n",
      "Epoch: 44, Step: 400:\n",
      "Epoch: 44, Step: 500:\n",
      "Epoch: 44, Step: 600:\n",
      "Epoch: 45, Step: 0:\n",
      "Epoch: 45, Step: 100:\n",
      "Epoch: 45, Step: 200:\n",
      "Epoch: 45, Step: 300:\n",
      "Epoch: 45, Step: 400:\n",
      "Epoch: 45, Step: 500:\n",
      "Epoch: 45, Step: 600:\n",
      "Epoch: 46, Step: 0:\n",
      "Epoch: 46, Step: 100:\n",
      "Epoch: 46, Step: 200:\n",
      "Epoch: 46, Step: 300:\n",
      "Epoch: 46, Step: 400:\n",
      "Epoch: 46, Step: 500:\n",
      "Epoch: 46, Step: 600:\n",
      "Epoch: 47, Step: 0:\n",
      "Epoch: 47, Step: 100:\n",
      "Epoch: 47, Step: 200:\n",
      "Epoch: 47, Step: 300:\n",
      "Epoch: 47, Step: 400:\n",
      "Epoch: 47, Step: 500:\n",
      "Epoch: 47, Step: 600:\n",
      "Epoch: 48, Step: 0:\n",
      "Epoch: 48, Step: 100:\n",
      "Epoch: 48, Step: 200:\n",
      "Epoch: 48, Step: 300:\n",
      "Epoch: 48, Step: 400:\n",
      "Epoch: 48, Step: 500:\n",
      "Epoch: 48, Step: 600:\n",
      "Epoch: 49, Step: 0:\n",
      "Epoch: 49, Step: 100:\n",
      "Epoch: 49, Step: 200:\n",
      "Epoch: 49, Step: 300:\n",
      "Epoch: 49, Step: 400:\n",
      "Epoch: 49, Step: 500:\n",
      "Epoch: 49, Step: 600:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pni/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type HierAttnNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/pni/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/pni/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/pni/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/pni/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/pni/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")\n",
    "for epoch in range(epochs):   # train e epochs\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # evety step, loader using one batch training data\n",
    "        output, at_weight = model(batch_x)\n",
    "        loss = loss_function(output, batch_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()            # clear gradients for this training step\n",
    "        loss.backward(retain_graph=True) # backpropagation, compute gradients\n",
    "        optimizer.step()                 # apply gradients\n",
    "        #exit()\n",
    "        if step % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                print(\"Epoch: %d, Step: %d:\" %(epoch, step))\n",
    "torch.save(model, \"./model_att/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# from statistics import median\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch.utils.data as Data\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "# from model import HierAttnNet\n",
    "\n",
    "\n",
    "# test_idx = [18,2049]\n",
    "\n",
    "\n",
    "# for doc in np.array(docs)[test_idx]:\n",
    "#     print(doc)\n",
    "#     print()\n",
    "    \n",
    "# X_seq_test = X_seq[test_idx]\n",
    "# x_test = torch.from_numpy(X_seq_test).to(torch.long)\n",
    "# GT = ycate[test_idx]\n",
    "\n",
    "\n",
    "# model = torch.load(\"./model_att/best\")\n",
    "# model.eval()\n",
    "# pred_distrib, att_weight = model(x_test) #log softmax \n",
    "# pred_y = torch.max(pred_distrib, 1)[1].data.cpu().numpy()\n",
    "# top3 = torch.topk(att_weight, k=3)[1].cpu().numpy()\n",
    "\n",
    "# for i in range(len(X_seq_test)):\n",
    "#     for idx in top3[i]:\n",
    "#         print('%0.4f' % (float(att_weight[i][idx])), end=' ')\n",
    "        \n",
    "#         if X_seq_test[i][idx] != 0:\n",
    "#             print(t.index_word[X_seq_test[i][idx]])\n",
    "#         else:\n",
    "#             print()\n",
    "#     print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nathaniel Woods executed in Alabama after Supreme Court denies stay\n",
      "\n",
      "0.2057 woods\n",
      "0.1069 \n",
      "0.1060 \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_title = 'China is recording so few new coronavirus infections that South Korea looks like the new center of the epidemic'\n",
    "input_title = 'Nathaniel Woods executed in Alabama after Supreme Court denies stay'\n",
    "print(input_title)\n",
    "print()\n",
    "\n",
    "X_seq_test =  pad_sequences(t.texts_to_sequences([input_title]), maxlen=max_length)\n",
    "x_test = torch.from_numpy(X_seq_test).to(torch.long)\n",
    "\n",
    "\n",
    "model = torch.load(\"./model_att/best\")\n",
    "model.eval()\n",
    "pred_distrib, att_weight = model(x_test) #log softmax \n",
    "pred_y = torch.max(pred_distrib, 1)[1].data.cpu().numpy()\n",
    "top3 = torch.topk(att_weight, k=3)[1].cpu().numpy()\n",
    "\n",
    "for i in range(len(X_seq_test)):   \n",
    "    for idx in top3[i]:\n",
    "        print('%0.4f' % (float(att_weight[i][idx])), end=' ')\n",
    "        \n",
    "        if X_seq_test[i][idx] != 0:\n",
    "            print(t.index_word[X_seq_test[i][idx]])\n",
    "        else:\n",
    "            print()\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
